{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoohiaShahzad_In_class_exercise_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roohiashahzad/Roohia_INFO5731_Spring2020/blob/main/RoohiaShahzad_In_class_exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo5bsEh2eeWi"
      },
      "source": [
        "# **The second In-class-exercise (1/27/2021, 20 points in total)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sPQ58dIeqgp"
      },
      "source": [
        "(1) Write a Python program to find the duplicate elements in a given array of integers. Return -1 If there are no such elements. (4 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnrvKMFTeoJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6a0361-0849-47e1-b80e-1e1bbb58e053"
      },
      "source": [
        "# write your answer here\n",
        "\n",
        "def findDuplicate(arrayOfInt):\n",
        "\n",
        "  holder = []\n",
        "\n",
        "  noDuplicate = -1\n",
        "\n",
        "  for i in range(len(arrayOfInt)):\n",
        "    if arrayOfInt[i] in holder:\n",
        "      return arrayOfInt[i]\n",
        "    else:\n",
        "      holder.append(arrayOfInt[i])\n",
        "\n",
        "  return noDuplicate\n",
        "\n",
        "  \n",
        "\n",
        "print(findDuplicate([1,2,3,4,4,5,6,6]))\n",
        "print(findDuplicate([1,2,3,4,5,6]))\n",
        "print(findDuplicate([1,2,3,4,5,6,6]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "-1\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYrH6n6IhZoQ"
      },
      "source": [
        "(2) Write a Python program to select all the Sundays of a specified year. (4 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSfPLd23eLpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922389ce-352c-4730-a74c-011b7331892d"
      },
      "source": [
        "# write your answer here\n",
        "from datetime import date, timedelta\n",
        "def findSundays(year):\n",
        "  # Jan 1 of year\n",
        "  dt = date(year, 1, 1)\n",
        "\n",
        "  #First Sunday\n",
        "  dt += timedelta(days = 7 - dt.isoweekday())\n",
        "  while dt.year == year:\n",
        "    yield dt\n",
        "    dt += timedelta(days = 7)\n",
        "\n",
        "for i in findSundays(2021):\n",
        "  print(i)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-03\n",
            "2021-01-10\n",
            "2021-01-17\n",
            "2021-01-24\n",
            "2021-01-31\n",
            "2021-02-07\n",
            "2021-02-14\n",
            "2021-02-21\n",
            "2021-02-28\n",
            "2021-03-07\n",
            "2021-03-14\n",
            "2021-03-21\n",
            "2021-03-28\n",
            "2021-04-04\n",
            "2021-04-11\n",
            "2021-04-18\n",
            "2021-04-25\n",
            "2021-05-02\n",
            "2021-05-09\n",
            "2021-05-16\n",
            "2021-05-23\n",
            "2021-05-30\n",
            "2021-06-06\n",
            "2021-06-13\n",
            "2021-06-20\n",
            "2021-06-27\n",
            "2021-07-04\n",
            "2021-07-11\n",
            "2021-07-18\n",
            "2021-07-25\n",
            "2021-08-01\n",
            "2021-08-08\n",
            "2021-08-15\n",
            "2021-08-22\n",
            "2021-08-29\n",
            "2021-09-05\n",
            "2021-09-12\n",
            "2021-09-19\n",
            "2021-09-26\n",
            "2021-10-03\n",
            "2021-10-10\n",
            "2021-10-17\n",
            "2021-10-24\n",
            "2021-10-31\n",
            "2021-11-07\n",
            "2021-11-14\n",
            "2021-11-21\n",
            "2021-11-28\n",
            "2021-12-05\n",
            "2021-12-12\n",
            "2021-12-19\n",
            "2021-12-26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIpziZ6Sjie-"
      },
      "source": [
        "(3) Python files reading and writing. Download the “[exercise_02_data _collection.zip](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/exercise_02_data_collection.zip)” to your local and un-zip it.\n",
        "\n",
        "*   Write a program to read all the txt files and save the sentences in all the files into one csv file with two columns, the first column is sentence id (txt file name+sentence line number), the second column is the sentence text content. (4 points)\n",
        "*   Remove all the punctuations from the sentences, save the processed sentences into a new column in the same csv file. (4 points)\n",
        "*   Ask the user to enter a word, return all the sentences that include this word, three kinds of information should be returned: sentence id, sentence text content, the count that user input word appear in the sentence. (4 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3CUsrWOj1Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20db3228-777a-4d78-fb0f-25e2d0da71b3"
      },
      "source": [
        "# write your answer here\n",
        "\n",
        "import csv\n",
        "with open('output.csv', \"w\", newline = \"\") as outputFile:\n",
        "  data_handler = csv.writer(outputFile, delimiter = ',')\n",
        "  i = 1\n",
        "  while i<=100:\n",
        "    inputFileName = \"/content/\" + \"ARTHROTEC.\" +str(i)+\".txt\"\n",
        "    outputLineName = \"ARTHROTEC\"+str(i)\n",
        "    inputFile = open(inputFileName).read()\n",
        "    sentences = inputFile.split('\\n')\n",
        "\n",
        "    for x in sentences:\n",
        "      if x =='':\n",
        "        sentences.remove(x)\n",
        "        \n",
        "    for x in sentences:\n",
        "      sentences_id = outputLineName+ \" Line \" + str(sentences.index(x))\n",
        "      data_handler.writerow([sentences_id, x])\n",
        "    \n",
        "    \n",
        "    i +=1\n",
        "\n",
        "\n",
        "\n",
        "import csv\n",
        "import re\n",
        "rowData = [] \n",
        "\n",
        "with open(\"output.csv\",'r') as inputFile:\n",
        "    contentReader = csv.reader(inputFile) \n",
        "    for row in contentReader:\n",
        "        new_sentence = re.sub(r\"[^a-zA-Z\\s\\']\",\"\",row[1]) \n",
        "        new_sentence = re.sub(\"\\s\\s\",\" \",new_sentence) \n",
        "        row.append(new_sentence) \n",
        "        rowData.append(row)\n",
        "\n",
        "with open(\"output.csv\",\"w\") as outputFile:\n",
        "    data_handler = csv.writer(outputFile,delimiter=',') \n",
        "    data_handler.writerows(rowData) \n",
        "\n",
        "\n",
        "import csv\n",
        "import re\n",
        "userInput = input(\"enter a word \") \n",
        "with open('output.csv','r') as inputFile:\n",
        "    contentReader = csv.reader(inputFile) \n",
        "    for row in contentReader: \n",
        "        word = \" \"+userInput+\" \"\n",
        "        if word in row[2]:\n",
        "            user_words = re.findall(word,row[2])\n",
        "            print(row[0],row[1],len(user_words))\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter a word headache\n",
            "ARTHROTEC7 Line 0 started having a severe sudden onset headache in the back of head, resulted in trip to DR, BP was extrememly high, and pulse was high as well, after three shots, and 3 clonidine to bring down bp was recommend to visit ER. 1\n",
            "ARTHROTEC16 Line 4 HOWEVER: I had a very bad headache yesterday and Ibuprofen again didn't work. 1\n",
            "ARTHROTEC42 Line 0 Stomach pains, chest pains, anxiety, fatigue, headache, erratic heart beat, raised blood pressure, dizziness, nausea (even when taken with food), blurred vision. 1\n",
            "ARTHROTEC80 Line 0 abdominal pain, constant soreness in abdomen, headache, general stomach upset feeling that won't go away before or after eating. 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-12cqmJEpZOh"
      },
      "source": [
        "(4) Install packages nltk, numpy, scipy, pandas, and sklearn on Google Colab. Write a program to test whether they are installed successfully. (3 points for extra)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1pn1Zl2qK77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd268c6-5cd6-4d7e-cc9a-c5fff85c7469"
      },
      "source": [
        "\n",
        "# NLTK\n",
        "\n",
        "! pip install nltk\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "test_text = \"Hello, world!\"\n",
        "print(wordpunct_tokenize(test_text))\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "# Numpy\n",
        "! pip install numpy\n",
        "import numpy as np\n",
        "n= np.arange(12)\n",
        "print(n)\n",
        "\n",
        "a_array = np.zeros((3,3))   \n",
        "print(a_array)\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "#scipy\n",
        "! pip install Scipy\n",
        "\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "arr = np.array([2,4,6,8,10,12])\n",
        "mean = arr.mean()\n",
        "sum = arr.sum()\n",
        "print (mean,sum)\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "#Pandas\n",
        "! pip install Pandas\n",
        "import pandas as pd\n",
        "dataset = {\n",
        "  'fruits': [\"apple\", \"orange\", \"melon\"],\n",
        "  'sales': [100, 240, 35]\n",
        "}\n",
        "\n",
        "mysales = pd.DataFrame(dataset)\n",
        "\n",
        "print(mysales)\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "#Sklearn\n",
        "! pip install Sklearn\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "print(iris.DESCR)\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "['Hello', ',', 'world', '!']\n",
            "----------------------------------------------------------------------\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.5)\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "----------------------------------------------------------------------\n",
            "Requirement already satisfied: Scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from Scipy) (1.19.5)\n",
            "7.0 42\n",
            "----------------------------------------------------------------------\n",
            "Requirement already satisfied: Pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from Pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from Pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from Pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->Pandas) (1.15.0)\n",
            "   fruits  sales\n",
            "0   apple    100\n",
            "1  orange    240\n",
            "2   melon     35\n",
            "----------------------------------------------------------------------\n",
            "Requirement already satisfied: Sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from Sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->Sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->Sklearn) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->Sklearn) (1.19.5)\n",
            ".. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 150 (50 in each of three classes)\n",
            "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            "        - sepal length in cm\n",
            "        - sepal width in cm\n",
            "        - petal length in cm\n",
            "        - petal width in cm\n",
            "        - class:\n",
            "                - Iris-Setosa\n",
            "                - Iris-Versicolour\n",
            "                - Iris-Virginica\n",
            "                \n",
            "    :Summary Statistics:\n",
            "\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "                    Min  Max   Mean    SD   Class Correlation\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
            "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
            "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
            "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: 33.3% for each of 3 classes.\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
            "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
            "Machine Learning Repository, which has two wrong data points.\n",
            "\n",
            "This is perhaps the best known database to be found in the\n",
            "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
            "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
            "data set contains 3 classes of 50 instances each, where each class refers to a\n",
            "type of iris plant.  One class is linearly separable from the other 2; the\n",
            "latter are NOT linearly separable from each other.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
            "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
            "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
            "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
            "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
            "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
            "     Structure and Classification Rule for Recognition in Partially Exposed\n",
            "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
            "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
            "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
            "     on Information Theory, May 1972, 431-433.\n",
            "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
            "     conceptual clustering system finds 3 classes in the data.\n",
            "   - Many, many more ...\n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}